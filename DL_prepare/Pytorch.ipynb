{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T00:19:18.170507Z",
     "start_time": "2020-07-03T00:19:18.162348Z"
    }
   },
   "source": [
    "This Tutorial is modified from [University of Washington CSE446](https://courses.cs.washington.edu/courses/cse446/19au/section9.html) and [PyTorch Official Tutorials](https://pytorch.org/tutorials/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Automatic differentiation is a powerful tool\n",
    "- PyTorch implements common functions used in deep learning\n",
    "- Data Processing with PyTorch DataSet\n",
    "- Mixed Presision Training in PyTorch (with Nvdia GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you are lazy, look at this [cheatsheet](https://hackmd.io/@rh0jTfFDTO6SteMDq91tgg/HkDRHKLrU#RNN-Recurrent-Neural-Networks) directly**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy and Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Pytorch is just Numpy on GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. tensor is basically an (multidimential) array.   \n",
    "2. Numpy methods generally have Pytorch alternatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor = Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T00:24:31.388033Z",
     "start_time": "2020-07-03T00:24:31.296380Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_numpy, x_torch\n",
      "[0.1 0.2 0.3] tensor([0.1000, 0.2000, 0.3000])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "# create tensors\n",
    "x_numpy = np.array([0.1,0.2,0.3])\n",
    "x_torch = torch.tensor([0.1,0.2,0.3]) # list embedded\n",
    "\n",
    "\n",
    "print('x_numpy, x_torch')\n",
    "print(x_numpy, x_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T00:26:40.222478Z",
     "start_time": "2020-07-03T00:26:40.215224Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1000, 0.2000, 0.3000], dtype=torch.float64)\n",
      "[0.1 0.2 0.3]\n"
     ]
    }
   ],
   "source": [
    "# transform btw numpy, torch\n",
    "print(torch.from_numpy(x_numpy)) # from numpy\n",
    "print(x_torch.numpy())   # to numpy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simple  manipulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "many functions that are in numpy are also in pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T00:30:20.288470Z",
     "start_time": "2020-07-03T00:30:20.281836Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm\n",
      "5.477225575051661 tensor(5.4772)\n"
     ]
    }
   ],
   "source": [
    "print(\"norm\")\n",
    "print(np.linalg.norm(x_numpy), torch.norm(x_torch))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some have same function but dif key words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T00:30:18.219379Z",
     "start_time": "2020-07-03T00:30:18.210261Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean along the 0th dimension\n",
      "[2. 3.] tensor([2., 3.])\n"
     ]
    }
   ],
   "source": [
    "print(\"mean along the 0th dimension\")\n",
    "x_numpy = np.array([[1,2],[3,4.]])\n",
    "x_torch = torch.tensor([[1,2],[3,4.]])\n",
    "print(np.mean(x_numpy, axis=0), torch.mean(x_torch, dim=0)) # dim vs axis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reshape tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Pytorch.view = Numpy.reshape`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy version : **reshape**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T22:44:57.449479Z",
     "start_time": "2020-07-06T22:44:57.288560Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 3, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "X2 = np.random.rand(10000, 3, 28, 28)\n",
    "print(X2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T22:45:05.706556Z",
     "start_time": "2020-07-06T22:45:05.700956Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 3, 784)\n"
     ]
    }
   ],
   "source": [
    "print(X2.reshape(N,C,28**2).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torch Version : **view**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T22:44:16.729181Z",
     "start_time": "2020-07-06T22:44:16.498679Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "X = torch.randn((10000, 3, 28, 28))\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T23:02:31.331246Z",
     "start_time": "2020-07-06T23:02:31.325570Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 3, 784])\n"
     ]
    }
   ],
   "source": [
    "# reshape torch\n",
    "print(X.view(10000,3,28**2).shape)   # use \"view\" built-in func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keep in mind that you did not actually **change the X** by Torch.View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T23:04:10.780905Z",
     "start_time": "2020-07-06T23:04:10.776095Z"
    }
   },
   "outputs": [],
   "source": [
    "# Unless you do this\n",
    "X = X.view(10000, 3, 28**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " >time-varying dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T23:02:36.760397Z",
     "start_time": "2020-07-06T23:02:36.754692Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 3, 784])\n"
     ]
    }
   ],
   "source": [
    "print(X.view(-1,3,784).shape)  # auto choose dim\n",
    "        # only one -1 required!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " starting at the trailing dimension, the dimension sizes must either be equal, one of them is 1, or one of them does not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T00:09:53.587785Z",
     "start_time": "2020-07-07T00:09:53.581624Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "x=torch.empty(5,1,4,1)\n",
    "y=torch.empty(  3,1,2)\n",
    "print((x+y).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T01:55:37.508672Z",
     "start_time": "2020-07-03T01:55:37.504620Z"
    }
   },
   "outputs": [],
   "source": [
    "# copy tensor btw GPU and CPU\n",
    "cpu = torch.device(\"cpu\")\n",
    "gpu = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T01:56:32.579427Z",
     "start_time": "2020-07-03T01:56:30.605413Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0569, 0.5231, 0.5170, 0.9659, 0.3097, 0.8913, 0.7250, 0.3659, 0.5064,\n",
      "        0.8923])\n",
      "tensor([0.0569, 0.5231, 0.5170, 0.9659, 0.3097, 0.8913, 0.7250, 0.3659, 0.5064,\n",
      "        0.8923], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(10)\n",
    "print(x)  # in cpu defualt\n",
    "\n",
    "x = x.to(gpu)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch AUTO Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the function $f(x) = (x-2)^2$.    \n",
    "Q: Compute 微分 $\\frac{d}{dx} f(x)$ and 微分值 then compute $f'(1)$.  \n",
    "\n",
    "function: **computing all the gradients of `y` at once.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T18:01:14.012549Z",
     "start_time": "2020-07-03T18:01:13.988910Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.])\n"
     ]
    }
   ],
   "source": [
    "# define func\n",
    "\n",
    "def f(x):\n",
    "    return (x-2)**2\n",
    "\n",
    "x = torch.tensor([1.0], requires_grad=True)\n",
    "\n",
    "y = f(x)\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a high-dimentional, complex func\n",
    "Let $w = [w_1, w_2]^T$\n",
    "\n",
    "Consider $g(w) = 2w_1w_2 + w_2\\cos(w_1)$\n",
    "\n",
    "Q: Compute $\\nabla_w g(w)$ and verify $\\nabla_w g([\\pi,1]) = [2, \\pi - 1]^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T18:12:52.045643Z",
     "start_time": "2020-07-03T18:12:52.015913Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.0000, 5.2832])\n"
     ]
    }
   ],
   "source": [
    "def g(w):\n",
    "    return 2*w[0]*w[1] + w[1]*torch.cos(w[0])\n",
    "\n",
    "w = torch.tensor([np.pi, 1], requires_grad=True) # at certain point\n",
    "\n",
    "z = g(w)\n",
    "z.backward()  # auto cal all gradient\n",
    "\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pytorch and gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $f$ the same function we defined above.\n",
    "\n",
    "Q: What is the value of $x$ that minimizes $f$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T19:46:04.216896Z",
     "start_time": "2020-07-03T19:46:04.197283Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter, \tx, \tf(x),  \tf'(x) pytorch\n",
      "0, \t5.000, \t9.000, \t6.000\n",
      "1, \t3.500, \t2.250, \t3.000\n",
      "2, \t2.750, \t0.562, \t1.500\n",
      "3, \t2.375, \t0.141, \t0.750\n",
      "4, \t2.188, \t0.035, \t0.375\n",
      "5, \t2.094, \t0.009, \t0.188\n",
      "6, \t2.047, \t0.002, \t0.094\n",
      "7, \t2.023, \t0.001, \t0.047\n",
      "8, \t2.012, \t0.000, \t0.023\n",
      "9, \t2.006, \t0.000, \t0.012\n",
      "10, \t2.003, \t0.000, \t0.006\n",
      "11, \t2.001, \t0.000, \t0.003\n",
      "12, \t2.001, \t0.000, \t0.001\n",
      "13, \t2.000, \t0.000, \t0.001\n",
      "14, \t2.000, \t0.000, \t0.000\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return (x-2)**2\n",
    "\n",
    "y = f(x)\n",
    "\n",
    "\n",
    "# begining point\n",
    "# x is tensor! \n",
    "x = torch.tensor([5.0], requires_grad=True)   # tensor start point = 5\n",
    "\n",
    "\n",
    "step_size = 0.25\n",
    "\n",
    "print('iter, \\tx, \\tf(x),  \\tf\\'(x) pytorch') # a str\n",
    "\n",
    "# move 15 steps\n",
    "for i in range(15):\n",
    "    y = f(x)\n",
    "    y.backward() # compute the gradient \n",
    "      \n",
    "                # precision = 3 decimal\n",
    "    print('{}, \\t{:.3f}, \\t{:.3f}, \\t{:.3f}'.format(i, x.item(), f(x).item(),  x.grad.item()))\n",
    "    \n",
    "    # x.data, not x\n",
    "    x.data = x.data - step_size * x.grad # perform a GD update step\n",
    "                                # x.grad must be calc separately if not apply torch\n",
    "        \n",
    "    # zero the grad variable each iteration since the backward()\n",
    "    # call accumulates the gradients in .grad instead of overwriting.\n",
    "    # The detach_() is for efficiency. \n",
    "    x.grad.detach_()\n",
    "    x.grad.zero_()\n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T17:52:53.665524Z",
     "start_time": "2020-07-04T17:52:53.660196Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.]\n"
     ]
    }
   ],
   "source": [
    "# print the last gradient (should be 0)\n",
    "print(x.grad.view(1).detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T19:51:47.382029Z",
     "start_time": "2020-07-03T19:51:47.368251Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x tensor([2.0001], requires_grad=True) y tensor([3.3528e-08], grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# simplify\n",
    "\n",
    "# define the func\n",
    "def f(x):\n",
    "    return (x-2)**2\n",
    "y = f(x)\n",
    "\n",
    "\n",
    "# define the tensor\n",
    "x = torch.tensor([5.0], requires_grad=True) \n",
    "\n",
    "\n",
    "# step size\n",
    "step_size = 0.25\n",
    "\n",
    "# update f(x)\n",
    "for i in range(15):\n",
    "    y = f(x)\n",
    "    y.backward()\n",
    "    x.data = x.data - step_size * x.grad\n",
    "    x.grad.detach_()\n",
    "    x.grad.zero_()\n",
    "\n",
    "print('x',x,'y',y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> when x = 2; we have y min = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "apply to Loss func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T20:01:09.012039Z",
     "start_time": "2020-07-03T20:01:09.002272Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape torch.Size([50, 2])\n",
      "y shape torch.Size([50, 1])\n",
      "w shape torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "# make a simpel linear dataset \n",
    "\n",
    "d = 2\n",
    "n = 50\n",
    "X = torch.randn(n, d)   # 2 vars\n",
    "\n",
    " # true parameters beta vector\n",
    "true_beta = torch.tensor([[-1.0], [2.0]])   \n",
    "        \n",
    "\n",
    "y = X @ true_beta + torch.randn(n, 1) * 0.1  # add noise\n",
    "\n",
    "\n",
    "# check\n",
    "print('X shape', X.shape)\n",
    "print('y shape', y.shape)\n",
    "print('w shape', true_beta.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check\n",
    "the gradient for the RSS objective:\n",
    "\n",
    "$$ \\widehat{y} = X\\beta$$\n",
    "\n",
    "$$\\nabla_w \\mathcal{L}_{RSS}(w; X) = \\nabla_\\beta\\frac{1}{n} ||y - \\widehat{y}||_2^2  $$   \n",
    "$$= -\\frac{2}{n}X^T(y-\\widehat{y})$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T17:46:00.190815Z",
     "start_time": "2020-07-05T17:46:00.180596Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first gradient [ 4.251404 -4.610432]\n"
     ]
    }
   ],
   "source": [
    "# define a linear model\n",
    "def model(X, beta):\n",
    "    return X @ beta\n",
    "\n",
    "# loss func: the residual (sum of square)\n",
    "def rss(y, y_hat):\n",
    "    return torch.norm(y - y_hat)**2 / n\n",
    "\n",
    "# set start point\n",
    "beta = torch.tensor([[1.],[0]], requires_grad=True)\n",
    "y_hat = model(X, beta)\n",
    "\n",
    "# cal gradient of rss\n",
    "loss = rss(y, y_hat)\n",
    "loss.backward()  # cal gradient for loss func\n",
    "\n",
    "print('first gradient', beta.grad.view(2).numpy())\n",
    "# the first "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> the gradient of the first update "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analytical Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T17:45:15.350921Z",
     "start_time": "2020-07-04T17:45:15.341214Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.2514043, -4.610432 ], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def analytic_rss(X, y, beta):\n",
    "    return -2 * X.t() @ (y - X @ beta) / n\n",
    "\n",
    "beta = torch.tensor([[1.],[0]], requires_grad=True) # begin with 1, 0 \n",
    "\n",
    "analytic_rss(X, y, beta).detach().view(2).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> should be the same as Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "redefine all functions to show the whole process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T20:37:33.305485Z",
     "start_time": "2020-07-04T20:37:33.264310Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter, \tloss, \tgrad, \tbeta\n",
      "0,\t8.897041, \t[[ 4.251404 -4.610432]], \t[0.5748596 0.4610432]\n",
      "1,\t5.399818, \t[[ 3.3413305 -3.5590112]], \t[0.24072656 0.81694436]\n",
      "2,\t3.280635, \t[[ 2.6282098 -2.745397 ]], \t[-0.02209443  1.0914841 ]\n",
      "3,\t1.996026, \t[[ 2.0690796 -2.1160963]], \t[-0.2290024  1.3030937]\n",
      "4,\t1.216986, \t[[ 1.6304004 -1.6296082]], \t[-0.39204246  1.4660544 ]\n",
      "5,\t0.744307, \t[[ 1.2859839 -1.2537354]], \t[-0.52064085  1.591428  ]\n",
      "6,\t0.457339, \t[[ 1.0153735  -0.96350896]], \t[-0.6221782  1.687779 ]\n",
      "7,\t0.282997, \t[[ 0.80258375 -0.73956835]], \t[-0.70243657  1.7617358 ]\n",
      "8,\t0.176992, \t[[ 0.63511825 -0.5669064 ]], \t[-0.7659484  1.8184265]\n",
      "9,\t0.112476, \t[[ 0.50320435 -0.43389428]], \t[-0.81626886  1.8618159 ]\n",
      "10,\t0.073165, \t[[ 0.39919543 -0.33152306]], \t[-0.8561884  1.8949683]\n",
      "11,\t0.049182, \t[[ 0.3171054 -0.2528169]], \t[-0.887899   1.9202499]\n",
      "12,\t0.034527, \t[[ 0.25224566 -0.1923753 ]], \t[-0.91312355  1.9394875 ]\n",
      "13,\t0.025557, \t[[ 0.2009418  -0.14602035]], \t[-0.9332177  1.9540895]\n",
      "14,\t0.020054, \t[[ 0.16031244 -0.11052071]], \t[-0.94924897  1.9651415 ]\n",
      "15,\t0.016671, \t[[ 0.1280962  -0.08337881]], \t[-0.9620586  1.9734794]\n",
      "16,\t0.014585, \t[[ 0.10251781 -0.0626652 ]], \t[-0.97231036  1.9797459 ]\n",
      "17,\t0.013295, \t[[ 0.08218179 -0.04689022]], \t[-0.98052853  1.9844348 ]\n",
      "18,\t0.012494, \t[[ 0.06599074 -0.03490498]], \t[-0.9871276  1.9879253]\n",
      "19,\t0.011995, \t[[ 0.05308065 -0.0258235 ]], \t[-0.9924357  1.9905076]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# define estimated model y_hat\n",
    "def model(X, beta):\n",
    "    return X @ beta\n",
    "\n",
    "\n",
    "\n",
    "# define loss func (rss in this case)\n",
    "def rss(y, y_hat):\n",
    "    return torch.norm(y - y_hat)**2 / n\n",
    "\n",
    "\n",
    "# define tensor  (parameters under estimation)\n",
    "beta = torch.tensor([[1.],[0]], requires_grad=True)\n",
    "\n",
    "\n",
    "############### GRADIENT DESCENT ###############\n",
    "step_size = 0.1\n",
    "\n",
    "\n",
    "# print 3 things (title)\n",
    "print('iter, \\tloss, \\tgrad, \\tbeta')\n",
    "\n",
    "# update (loop)\n",
    "   # update 20 times\n",
    "\n",
    "for i in range(20):\n",
    "    \n",
    "    # redefine every iteration\n",
    "    y_hat = model(X, beta)\n",
    "    loss = rss(y, y_hat)\n",
    "\n",
    "    \n",
    "    \n",
    "    # step 1: cal gradient \n",
    "    loss.backward() # auto apply to tensor (which is beta)\n",
    "    \n",
    "    # step 2: update para\n",
    "    beta.data = beta.data - step_size * beta.grad \n",
    "    #print(beta.grad.numpy())\n",
    "    \n",
    "    print('{},\\t{:2f}, \\t{}, \\t{}'.format(i, loss.item(), beta.grad.numpy().transpose(), beta.view(2).detach().numpy().transpose()))\n",
    "    \n",
    "    # zero grad since backward\n",
    "    # accumulates gradients instead of overwrtite\n",
    "    \n",
    "    beta.grad.detach()# for efficiency\n",
    "    beta.grad.zero_()\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> beta = [-1,2]; very close !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T21:34:10.959277Z",
     "start_time": "2020-07-04T21:34:10.950434Z"
    }
   },
   "source": [
    "`Module` is PyTorch's way of performing operations on tensors. Modules are implemented as subclasses of the `torch.nn.Module` class. All modules are callable and can be composed together to create complex functions.\n",
    "\n",
    "[`torch.nn` docs](https://pytorch.org/docs/stable/nn.html)\n",
    "\n",
    "Note: most of the functionality implemented for modules can be accessed in a functional form via `torch.nn.functional`, but these require you to create and manage the weight tensors yourself.\n",
    "\n",
    "[`torch.nn.functional` docs](https://pytorch.org/docs/stable/nn.html#torch-nn-functional)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T21:39:41.726931Z",
     "start_time": "2020-07-04T21:39:41.717587Z"
    }
   },
   "source": [
    "## Linear Transformation\n",
    "Unlike how we initialized our $\\beta$ manually, the Linear module automatically initializes the **weights** randomly.\n",
    "\n",
    "[`torch.nn.init` docs](https://pytorch.org/docs/stable/nn.html#torch-nn-init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear module does a linear transformation with a bias.    \n",
    "It takes the input and output dim as para, then creates the weights of the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T17:50:13.817405Z",
     "start_time": "2020-07-05T17:50:13.813499Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see [documentation](https://pytorch.org/docs/master/generated/torch.nn.Linear.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T22:15:20.952577Z",
     "start_time": "2020-07-06T22:15:20.940205Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n",
      "out tensor([[-0.6987, -0.4822,  0.1826],\n",
      "        [-0.0055,  0.3998, -0.5024],\n",
      "        [-0.5060, -0.2308,  0.0036],\n",
      "        [-0.1410,  0.0688, -0.6597],\n",
      "        [-0.3254,  0.0573, -0.0678],\n",
      "        [ 0.2328,  0.8916, -0.3916],\n",
      "        [-1.5062, -1.3886,  1.2027],\n",
      "        [-0.4364, -0.4086, -0.5541],\n",
      "        [-0.8600, -0.8621,  0.0213]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# take in, out dim as para\n",
    "m = nn.Linear(2, 3) # as a func\n",
    "\n",
    "\n",
    "# an example (dim = 3)\n",
    "input = torch.randn(9, 2)\n",
    "\n",
    "# apply a linear transformation to the data\n",
    "transformed = m(input)\n",
    "\n",
    "\n",
    "\n",
    "print('in', example_tensor)\n",
    "print('out',transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch implements a number of activation functions including but not limited to `ReLU`, `Tanh`, and `Sigmoid`. Since they are modules, they need to be instantiated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T22:20:04.250580Z",
     "start_time": "2020-07-06T22:20:04.244845Z"
    }
   },
   "outputs": [],
   "source": [
    "# define active func\n",
    "activ_fn = nn.Sigmoid()\n",
    "\n",
    "# input tensor\n",
    "ex_tensor = torch.tensor([-1,1,0])\n",
    "\n",
    "# \n",
    "activated  = activ_fn(ex_tensor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
